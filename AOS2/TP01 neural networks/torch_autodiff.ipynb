{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch autodiff introduction\n",
    "## Autodiff\n",
    "Load needed libraries\n",
    "$\\newcommand\\p[1]{{\\left(#1\\right)}}$\n",
    "$\\newcommand\\code[1]{\\texttt{#1}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple example of how to find the minimum of the function\n",
    "$x\\mapsto\\p{x-3}^2$ using the autodiff functionality of Pytorch.\n",
    "\n",
    "First initialize a tensor `x` and indicate that we want to store a\n",
    "gradient on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an optimizer on parameters. Here we want to optimize w.r.t.\n",
    "variable `x`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([x], lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Create a computational graph using parameters (here only `x`) and\n",
    "potentially other tensors.\n",
    "\n",
    "Here we only want to compute $\\p{x-3}^2$ so we define:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = (x - 3) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back-propagating gradients for `y` down to `x`. Don't forget to\n",
    "reset gradients before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use gradient on `x` to apply a one-step gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0400], requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And last we iterate the whole process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, x: 1.040000, loss: 4.000000\n",
      "Iteration: 20, x: 1.691488, loss: 1.782802\n",
      "Iteration: 40, x: 2.126427, loss: 0.794596\n",
      "Iteration: 60, x: 2.416796, loss: 0.354152\n",
      "Iteration: 80, x: 2.610648, loss: 0.157846\n",
      "Iteration: 100, x: 2.740066, loss: 0.070352\n",
      "Iteration: 120, x: 2.826466, loss: 0.031356\n",
      "Iteration: 140, x: 2.884147, loss: 0.013975\n",
      "Iteration: 160, x: 2.922656, loss: 0.006229\n",
      "Iteration: 180, x: 2.948364, loss: 0.002776\n",
      "Iteration: 200, x: 2.965527, loss: 0.001237\n",
      "Iteration: 220, x: 2.976985, loss: 0.000552\n",
      "Iteration: 240, x: 2.984635, loss: 0.000246\n",
      "Iteration: 260, x: 2.989742, loss: 0.000110\n",
      "Iteration: 280, x: 2.993152, loss: 0.000049\n",
      "Iteration: 300, x: 2.995428, loss: 0.000022\n",
      "Iteration: 320, x: 2.996948, loss: 0.000010\n",
      "Iteration: 340, x: 2.997962, loss: 0.000004\n",
      "Iteration: 360, x: 2.998639, loss: 0.000002\n",
      "Iteration: 380, x: 2.999091, loss: 0.000001\n",
      "Iteration: 400, x: 2.999393, loss: 0.000000\n",
      "Iteration: 420, x: 2.999595, loss: 0.000000\n",
      "Iteration: 440, x: 2.999730, loss: 0.000000\n",
      "Iteration: 460, x: 2.999820, loss: 0.000000\n",
      "Iteration: 480, x: 2.999880, loss: 0.000000\n",
      "Iteration: 500, x: 2.999919, loss: 0.000000\n",
      "Iteration: 520, x: 2.999946, loss: 0.000000\n",
      "Iteration: 540, x: 2.999964, loss: 0.000000\n",
      "Iteration: 560, x: 2.999976, loss: 0.000000\n",
      "Iteration: 580, x: 2.999984, loss: 0.000000\n",
      "Iteration: 600, x: 2.999989, loss: 0.000000\n",
      "Iteration: 620, x: 2.999993, loss: 0.000000\n",
      "Iteration: 640, x: 2.999994, loss: 0.000000\n",
      "Iteration: 660, x: 2.999994, loss: 0.000000\n",
      "Iteration: 680, x: 2.999994, loss: 0.000000\n",
      "Iteration: 700, x: 2.999994, loss: 0.000000\n",
      "Iteration: 720, x: 2.999994, loss: 0.000000\n",
      "Iteration: 740, x: 2.999994, loss: 0.000000\n",
      "Iteration: 760, x: 2.999994, loss: 0.000000\n",
      "Iteration: 780, x: 2.999994, loss: 0.000000\n",
      "Iteration: 800, x: 2.999994, loss: 0.000000\n",
      "Iteration: 820, x: 2.999994, loss: 0.000000\n",
      "Iteration: 840, x: 2.999994, loss: 0.000000\n",
      "Iteration: 860, x: 2.999994, loss: 0.000000\n",
      "Iteration: 880, x: 2.999994, loss: 0.000000\n",
      "Iteration: 900, x: 2.999994, loss: 0.000000\n",
      "Iteration: 920, x: 2.999994, loss: 0.000000\n",
      "Iteration: 940, x: 2.999994, loss: 0.000000\n",
      "Iteration: 960, x: 2.999994, loss: 0.000000\n",
      "Iteration: 980, x: 2.999994, loss: 0.000000\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "y = (x - 3) ** 2\n",
    "optimizer = optim.SGD([x], lr=0.01)\n",
    "it = 0\n",
    "while it < 1000:\n",
    "    loss = (x - 3) ** 2\n",
    "    # set gradient to zero, else it will accumulate the gradients\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if it % 20 == 0:\n",
    "        print('Iteration: %d, x: %f, loss: %f' % (it, x.item(), loss.item()))\n",
    "    it += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Differentiate the exponential\n",
    "The exponential function can be approximated using its Taylor\n",
    "expansion:\n",
    "\\\\[\\exp\\p{z}\\approx\\sum_{k=0}^{N}\\frac{z^k}{k!}\\\\]\n",
    "\n",
    "First define `z`, the \"parameter\" and build a computational graph\n",
    "from it to compute the exponential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.tensor([1.0], requires_grad=True)\n",
    "N = 10\n",
    "fk = 1\n",
    "zk = 1\n",
    "expz = 0\n",
    "for k in range(N):\n",
    "    expz = expz + zk/fk\n",
    "    zk = zk * z\n",
    "    fk = fk * (k + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the gradient and verify that it is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.7183])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expz.backward()\n",
    "z.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.718281828459045"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving equations with Pytorch\n",
    "Suppose we want to solve the following system of two equations\n",
    "\\\\[e^{-e^{-(x_1 + x_2)}} = x_2 (1 + x_1^2)\\\\]\n",
    "\\\\[x_1 \\cos(x_2) + x_2 \\sin(x_1) = 1/2\\\\]\n",
    "\n",
    "Find a loss whose optimization leads to a solution of the system of\n",
    "equations above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x1, x2):\n",
    "    return torch.exp(-torch.exp(-(x1 + x2))) - x2 * (1 + x1 ** 2)\n",
    "def f2(x1, x2):\n",
    "    return x1 * torch.cos(x2) + x2 * torch.sin(x1) - 0.5\n",
    "x1 = torch.tensor([0.0], requires_grad=True)\n",
    "x2 = torch.tensor([0.0], requires_grad=True)\n",
    "\n",
    "loss = f1(x1, x2)**2 + f2(x1, x2)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Pytorch autodiff to solve the system of equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, x1: 0.007293, x2: 0.004651, loss: 0.385335\n",
      "Iteration: 20, x1: 0.134238, x2: 0.107664, loss: 0.250012\n",
      "Iteration: 40, x1: 0.229074, x2: 0.214341, loss: 0.146439\n",
      "Iteration: 60, x1: 0.294770, x2: 0.309024, loss: 0.078318\n",
      "Iteration: 80, x1: 0.335219, x2: 0.384967, loss: 0.040020\n",
      "Iteration: 100, x1: 0.356608, x2: 0.442270, loss: 0.020582\n",
      "Iteration: 120, x1: 0.365716, x2: 0.484327, loss: 0.010968\n",
      "Iteration: 140, x1: 0.368040, x2: 0.515040, loss: 0.006060\n",
      "Iteration: 160, x1: 0.367179, x2: 0.537610, loss: 0.003429\n",
      "Iteration: 180, x1: 0.365141, x2: 0.554350, loss: 0.001966\n",
      "Iteration: 200, x1: 0.362903, x2: 0.566874, loss: 0.001134\n",
      "Iteration: 220, x1: 0.360872, x2: 0.576303, loss: 0.000656\n",
      "Iteration: 240, x1: 0.359175, x2: 0.583435, loss: 0.000380\n",
      "Iteration: 260, x1: 0.357814, x2: 0.588844, loss: 0.000221\n",
      "Iteration: 280, x1: 0.356749, x2: 0.592954, loss: 0.000128\n",
      "Iteration: 300, x1: 0.355924, x2: 0.596081, loss: 0.000074\n",
      "Iteration: 320, x1: 0.355290, x2: 0.598461, loss: 0.000043\n",
      "Iteration: 340, x1: 0.354805, x2: 0.600274, loss: 0.000025\n",
      "Iteration: 360, x1: 0.354435, x2: 0.601656, loss: 0.000015\n",
      "Iteration: 380, x1: 0.354152, x2: 0.602708, loss: 0.000008\n",
      "Iteration: 400, x1: 0.353937, x2: 0.603510, loss: 0.000005\n",
      "Iteration: 420, x1: 0.353773, x2: 0.604122, loss: 0.000003\n",
      "Iteration: 440, x1: 0.353648, x2: 0.604588, loss: 0.000002\n",
      "Iteration: 460, x1: 0.353552, x2: 0.604943, loss: 0.000001\n",
      "Iteration: 480, x1: 0.353480, x2: 0.605214, loss: 0.000001\n",
      "Iteration: 500, x1: 0.353424, x2: 0.605420, loss: 0.000000\n",
      "Iteration: 520, x1: 0.353382, x2: 0.605577, loss: 0.000000\n",
      "Iteration: 540, x1: 0.353350, x2: 0.605697, loss: 0.000000\n",
      "Iteration: 560, x1: 0.353325, x2: 0.605789, loss: 0.000000\n",
      "Iteration: 580, x1: 0.353307, x2: 0.605858, loss: 0.000000\n",
      "Iteration: 600, x1: 0.353292, x2: 0.605912, loss: 0.000000\n",
      "Iteration: 620, x1: 0.353281, x2: 0.605952, loss: 0.000000\n",
      "Iteration: 640, x1: 0.353273, x2: 0.605983, loss: 0.000000\n",
      "Iteration: 660, x1: 0.353267, x2: 0.606006, loss: 0.000000\n",
      "Iteration: 680, x1: 0.353262, x2: 0.606024, loss: 0.000000\n",
      "Iteration: 700, x1: 0.353258, x2: 0.606038, loss: 0.000000\n",
      "Iteration: 720, x1: 0.353256, x2: 0.606048, loss: 0.000000\n",
      "Iteration: 740, x1: 0.353254, x2: 0.606056, loss: 0.000000\n",
      "Iteration: 760, x1: 0.353252, x2: 0.606062, loss: 0.000000\n",
      "Iteration: 780, x1: 0.353251, x2: 0.606067, loss: 0.000000\n",
      "Iteration: 800, x1: 0.353250, x2: 0.606071, loss: 0.000000\n",
      "Iteration: 820, x1: 0.353249, x2: 0.606073, loss: 0.000000\n",
      "Iteration: 840, x1: 0.353249, x2: 0.606075, loss: 0.000000\n",
      "Iteration: 860, x1: 0.353248, x2: 0.606076, loss: 0.000000\n",
      "Iteration: 880, x1: 0.353248, x2: 0.606078, loss: 0.000000\n",
      "Iteration: 900, x1: 0.353248, x2: 0.606079, loss: 0.000000\n",
      "Iteration: 920, x1: 0.353247, x2: 0.606079, loss: 0.000000\n",
      "Iteration: 940, x1: 0.353247, x2: 0.606079, loss: 0.000000\n",
      "Iteration: 960, x1: 0.353247, x2: 0.606079, loss: 0.000000\n",
      "Iteration: 980, x1: 0.353247, x2: 0.606079, loss: 0.000000\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD([x1, x2], lr=0.01)\n",
    "it = 0\n",
    "while it < 1000:\n",
    "    loss = f1(x1, x2)**2 + f2(x1, x2)**2\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if it % 20 == 0:\n",
    "        print('Iteration: %d, x1: %f, x2: %f, loss: %f' % (it, x1.item(), x2.item(), loss.item()))\n",
    "    it += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
