{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch autodiff introduction\n",
    "## Autodiff\n",
    "Load needed libraries\n",
    "$\\newcommand\\p[1]{{\\left(#1\\right)}}$\n",
    "$\\newcommand\\code[1]{\\texttt{#1}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple example of how to find the minimum of the function\n",
    "$x\\mapsto\\p{x-3}^2$ using the autodiff functionality of Pytorch.\n",
    "\n",
    "First initialize a tensor `x` and indicate that we want to store a\n",
    "gradient on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an optimizer on parameters. Here we want to optimize w.r.t.\n",
    "variable `x`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([x], lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Create a computational graph using parameters (here only `x`) and\n",
    "potentially other tensors.\n",
    "\n",
    "Here we only want to compute $\\p{x-3}^2$ so we define:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = (x - 3) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back-propagating gradients for `y` down to `x`. Don't forget to\n",
    "reset gradients before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use gradient on `x` to apply a one-step gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()\n",
    "x.grad\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And last we iterate the whole process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "it = 0\n",
    "while it < 1000:\n",
    "    loss = (x - 3) ** 2\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if it % 20 == 0:\n",
    "        print('Iteration: %d, x: %f, loss: %f' % (it, x.item(), loss.item()))\n",
    "    it += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Differentiate the exponential\n",
    "The exponential function can be approximated using its Taylor\n",
    "expansion:\n",
    "\\\\[\\exp\\p{z}\\approx\\sum_{k=0}^{N}\\frac{z^k}{k!}\\\\]\n",
    "\n",
    "First define `x`, the \"parameter\" and build a computational graph\n",
    "from it to compute the exponential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the gradient and verify that it is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving equations with Pytorch\n",
    "Suppose we want to solve the following system of two equations\n",
    "\\\\[e^{-e^{-(x_1 + x_2)}} = x_2 (1 + x_1^2)\\\\]\n",
    "\\\\[x_1 \\cos(x_2) + x_2 \\sin(x_1) = 1/2\\\\]\n",
    "\n",
    "Find a loss whose optimization leads to a solution of the system of\n",
    "equations above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Pytorch autodiff to solve the system of equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
