{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "pytorch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.11"
    },
    "colab": {
      "name": "sentiment_analysis.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbKmwkv4LiJd"
      },
      "source": [
        "# Word embedding and RNN for sentiment analysis\n",
        "\n",
        "The goal of the following notebook is to predict whether a written\n",
        "critic about a movie is positive or negative. For that we will try\n",
        "three models. A simple linear model on the word embeddings, a\n",
        "recurrent neural network and a CNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGVgoUnJLiJk"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchtext import data, datasets, vocab\n",
        "\n",
        "# Used to cache pretrained embeddings\n",
        "import appdirs"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrfxEOjdLiJm",
        "outputId": "fe4e40e1-e9d1-4243-d5df-3171d04d6148"
      },
      "source": [
        "!pip install appdirs"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting appdirs\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/00/2344469e2084fb287c2e0b57b72910309874c3245463acd6cf5e3db69324/appdirs-1.4.4-py2.py3-none-any.whl\n",
            "Installing collected packages: appdirs\n",
            "Successfully installed appdirs-1.4.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uROkNHs_LiJn"
      },
      "source": [
        "## Download data\n",
        "\n",
        "First run the following block that will install spacy for tokenizing\n",
        "and download an IMDB dataset and a GloVe embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7wNu9utLiJo",
        "outputId": "248d5f21-1e63-41c9-d7b1-03dcc7b9de60"
      },
      "source": [
        "!pip install --user spacy\n",
        "!spacy download en --user\n",
        "\n",
        "# Download IMDB data\n",
        "torch_cache = appdirs.user_cache_dir(\"pytorch\")\n",
        "datasets.IMDB.download(torch_cache)\n",
        "\n",
        "# Download GloVe word embedding\n",
        "vocab.GloVe(name=\"6B\", dim=\"100\", cache=torch_cache)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (50.3.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (2.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.0)\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (50.3.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.11.8)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:03<00:00, 21.9MB/s]\n",
            "/root/.cache/pytorch/glove.6B.zip: 862MB [06:28, 2.22MB/s]                           \n",
            "100%|█████████▉| 398365/400000 [00:19<00:00, 20764.26it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torchtext.vocab.GloVe at 0x7fae049a6278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eep6woAzLiJp"
      },
      "source": [
        "## Global variables\n",
        "\n",
        "First let's define a few variables. `vocab_size` is the size of the\n",
        "vocabulary (ie number of known words) we will use. `embedding_dim` is\n",
        "the dimension of the vector space used to embed all the words of the\n",
        "vocabulary. `seq_length` is the maximum length of a sequence,\n",
        "`batch_size` is the size of the batches used in stochastic\n",
        "optimization algorithms and `n_epochs` the number of times we are\n",
        "going thought the entire training set during the training phase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "tauXQ3-cLiJq"
      },
      "source": [
        "# Define a few variables\n",
        "vocab_size = 10000\n",
        "embedding_dim = 100\n",
        "seq_length = 64\n",
        "batch_size = 128\n",
        "n_epochs = 10"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2UJE0LZLiJq"
      },
      "source": [
        "## The IMDB dataset\n",
        "\n",
        "We use SpaCy and torchtext to create training, validation and testing\n",
        "datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8IJsvmtLiJq",
        "outputId": "0eb9c59d-28eb-4b65-8f60-16f0231f4f43"
      },
      "source": [
        "import spacy\n",
        "\n",
        "spacy_en = spacy.load(\"en\")\n",
        "tokenize = \"spacy\"\n",
        "\n",
        "# Declare the fields\n",
        "TEXT = data.Field(\n",
        "    tokenize=tokenize, fix_length=seq_length, lower=True, batch_first=True\n",
        ")\n",
        "LABEL = data.LabelField(sequential=False, dtype=torch.float)\n",
        "\n",
        "# IMDB dataset is already divided into train and test\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL, root=torch_cache)\n",
        "\n",
        "# Creating a validation dataset from the training one\n",
        "train_data, valid_data = train_data.split(split_ratio=0.8)\n",
        "\n",
        "print(f\"Number of training examples: {len(train_data)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data)}\")\n",
        "print(f\"Number of testing examples: {len(test_data)}\")\n",
        "\n",
        "TEXT.build_vocab(train_data, max_size=vocab_size)\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 20000\n",
            "Number of validation examples: 5000\n",
            "Number of testing examples: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivN_JbsmLiJr"
      },
      "source": [
        "# Define true vocabulary size because there are two more tokens\n",
        "vocab_size_ = len(TEXT.vocab.stoi)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 2,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tm4mxNO6LiJs",
        "outputId": "c6d05f57-1757-40bf-aa86-efcc979dab0f"
      },
      "source": [
        "print(len(TEXT.vocab))\n",
        "print(TEXT.vocab.itos[:10])\n",
        "print(train_data.examples[0].text[:seq_length])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10002\n",
            "['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is']\n",
            "['worst', 'movie', 'on', 'earth', '.', 'i', 'do', \"n't\", 'even', 'know', 'where', 'to', 'begin', 'but', 'i', 'hope', 'i', 'can', 'save', 'another', 'person', 'from', 'punishing', 'themselves', 'with', 'this', 'movie', '.', 'when', 'it', 'comes', 'to', 'acting', 'and', 'lighting', ',', 'this', 'movie', 'is', 'similar', 'to', 'a', 'bad', 'porno', 'without', 'the', 'sex', '.', 'the', 'actors', 'are', 'some', 'of', 'the', 'worst', 'i', \"'ve\", 'ever', 'seen', ',', 'and', 'could', \"n't\", 'have']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJ03Pl2tP34q",
        "outputId": "a12e04a7-2d65-458b-fb94-7241da43d2e0"
      },
      "source": [
        "train_data.fields"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label': <torchtext.data.field.LabelField at 0x7fadfac7ca20>,\n",
              " 'text': <torchtext.data.field.Field at 0x7fadfac7b2e8>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "Ut_Y7qvTLiJt"
      },
      "source": [
        "## Training a linear classifier with an embedding\n",
        "\n",
        "We first test a simple linear classifier on the word embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3rltc9DLiJt"
      },
      "source": [
        "class EmbeddingNet(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, seq_length):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.seq_length = seq_length\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # Define an embedding of `vocab_size` words into a vector space\n",
        "        # of dimension `embedding_dim`.\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size_, embedding_dim)\n",
        "\n",
        "\n",
        "        # Define a linear layer from dimension `seq_length` *\n",
        "        # `embedding_dim` to 1.\n",
        "\n",
        "        self.l1 = nn.Linear(seq_length * embedding_dim, 1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # `x` is of size `batch_size` * `seq_length`\n",
        "\n",
        "        # Compute the embedding `embedded` of the batch `x`. `embedded` is\n",
        "        # of size `batch_size` * `seq_length` * `embedding_dim`\n",
        "\n",
        "        embedded = self.embedding(x)\n",
        "\n",
        "\n",
        "        # Flatten the embedded words and feed it to the linear layer.\n",
        "        # `flatten` is of size `batch_size` * (`seq_length` * `embedding_dim`)\n",
        "\n",
        "        flatten = embedded.view(-1, seq_length * embedding_dim)\n",
        "\n",
        "\n",
        "        # Apply the linear layer and return a squeezed version\n",
        "        # `l1` is of size `batch_size`\n",
        "        # Return a tensor of size `batch size`\n",
        "        return self.l1(flatten).squeeze()\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "tS04DBnALiJu"
      },
      "source": [
        "We need to implement an accuracy function to be used in the `Trainer`\n",
        "class (see below)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77gRPd1KLiJu"
      },
      "source": [
        "def accuracy(predictions, labels):\n",
        "    # `predictions` and `labels` are both tensors of same length\n",
        "\n",
        "    # Implement accuracy\n",
        "    return torch.sum((torch.sigmoid(predictions) > .5).float() == labels).item() / len(predictions)\n",
        "\n",
        "\n",
        "\n",
        "assert accuracy(torch.Tensor([1, -2, 3]), torch.Tensor([1, 0, 1])) == 1\n",
        "assert accuracy(torch.Tensor([1, -2, -3]), torch.Tensor([1, 0, 1])) == 2 / 3"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cI3JQ1vzVG-M",
        "outputId": "5756740c-57c8-4157-a459-2529ab936ef4"
      },
      "source": [
        "torch.Tensor([1, -2, 3]).size()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRX2je_yLiJu"
      },
      "source": [
        "We implement now a `Trainer` class that takes care of the learning\n",
        "process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 2,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqxDekm2LiJu",
        "outputId": "62ac7bf5-f727-4f39-d701-8f71a7476171"
      },
      "source": [
        "class Trainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_epochs=10,\n",
        "        model=None,\n",
        "        optimizer=None,\n",
        "        criterion=None,\n",
        "        train_data=None,\n",
        "        valid_data=None,\n",
        "        test_data=None,\n",
        "        batch_size=None,\n",
        "    ):\n",
        "        self.n_epochs = n_epochs\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion\n",
        "        self.train_iterator = data.BucketIterator(train_data, batch_size=batch_size)\n",
        "        self.test_data = test_data\n",
        "        self.valid_data = valid_data\n",
        "\n",
        "    def train(self):\n",
        "        # model might have changed so redefine `optimizer`\n",
        "        optimizer = self.optimizer(self.model.parameters())\n",
        "\n",
        "        for epoch in range(1, self.n_epochs + 1):\n",
        "            print(f\"Epoch {epoch}/{self.n_epochs}\")\n",
        "\n",
        "            running_loss = 0.0\n",
        "            self.model.train()  # turn on training mode\n",
        "            for step, batch in enumerate(self.train_iterator):\n",
        "                optimizer.zero_grad()\n",
        "                predictions = self.model(batch.text)\n",
        "                loss = self.criterion(predictions, batch.label)\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "                if step % 150 == 0:\n",
        "                    acc = accuracy(predictions, batch.label)\n",
        "                    print(f\"Loss: {loss.item()/batch_size}, Accuracy {acc}\")\n",
        "\n",
        "            epoch_loss = running_loss / len(train_data)\n",
        "\n",
        "            # Calculate the validation loss for this epoch\n",
        "            self.model.eval()  # turn on evaluation mode\n",
        "            full_batch = data.Batch(self.valid_data, self.valid_data)\n",
        "            # Define the accuracy `valid_acc` and the loss `valid_loss` on `full_batch`\n",
        "            predictions = self.model(full_batch.text)\n",
        "            valid_loss = criterion(predictions, full_batch.label)/len(predictions)\n",
        "            valid_acc = accuracy(predictions, full_batch.label)\n",
        "            print(\n",
        "                f\"Epoch: {epoch}, Training Loss: {epoch_loss:.4f}, Validation Loss: {valid_loss:.4f}, Validation accuracy: {valid_acc:.4f}\"\n",
        "            )\n",
        "\n",
        "    def test(self):\n",
        "        with torch.no_grad():\n",
        "            self.model.eval()  # turn on evaluation mode\n",
        "            full_batch = data.Batch(self.test_data, self.test_data)\n",
        "            # Define the accuracy `test_acc` and the loss `test_loss` on `full_batch`\n",
        "            predictions = self.model(full_batch.text)\n",
        "            test_loss = self.criterion(predictions, full_batch.label)/len(predictions)\n",
        "            test_acc = accuracy(predictions, full_batch.label)\n",
        "\n",
        "\n",
        "            print(f\"Test Loss: {test_loss:.4f}, Test accuracy: {test_acc:.4f}\")\n",
        "\n",
        "\n",
        "embedding_net = EmbeddingNet(vocab_size_, embedding_dim, seq_length)\n",
        "optimizer = optim.Adam\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "tr = Trainer(\n",
        "    n_epochs=n_epochs,\n",
        "    model=embedding_net,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    train_data=train_data,\n",
        "    test_data=test_data,\n",
        "    valid_data=valid_data,\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "\n",
        "tr.train()\n",
        "tr.test()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "Loss: 0.005551036447286606, Accuracy 0.5234375\n",
            "Loss: 0.0054849740117788315, Accuracy 0.5703125\n",
            "Epoch: 1, Training Loss: 0.0057, Validation Loss: 0.0001, Validation accuracy: 0.5020\n",
            "Epoch 2/10\n",
            "Loss: 0.005908539518713951, Accuracy 0.46875\n",
            "Loss: 0.005519460421055555, Accuracy 0.5546875\n",
            "Epoch: 2, Training Loss: 0.0058, Validation Loss: 0.0001, Validation accuracy: 0.5020\n",
            "Epoch 3/10\n",
            "Loss: 0.005758148152381182, Accuracy 0.4765625\n",
            "Loss: 0.005971439648419619, Accuracy 0.5078125\n",
            "Epoch: 3, Training Loss: 0.0058, Validation Loss: 0.0001, Validation accuracy: 0.5020\n",
            "Epoch 4/10\n",
            "Loss: 0.005478661973029375, Accuracy 0.515625\n",
            "Loss: 0.0056166802532970905, Accuracy 0.46875\n",
            "Epoch: 4, Training Loss: 0.0058, Validation Loss: 0.0001, Validation accuracy: 0.5020\n",
            "Epoch 5/10\n",
            "Loss: 0.005516647361218929, Accuracy 0.4921875\n",
            "Loss: 0.005887059029191732, Accuracy 0.4296875\n",
            "Epoch: 5, Training Loss: 0.0058, Validation Loss: 0.0001, Validation accuracy: 0.5020\n",
            "Epoch 6/10\n",
            "Loss: 0.005888971965759993, Accuracy 0.46875\n",
            "Loss: 0.00545703386887908, Accuracy 0.625\n",
            "Epoch: 6, Training Loss: 0.0058, Validation Loss: 0.0001, Validation accuracy: 0.5020\n",
            "Epoch 7/10\n",
            "Loss: 0.005418732762336731, Accuracy 0.5390625\n",
            "Loss: 0.005609460175037384, Accuracy 0.5\n",
            "Epoch: 7, Training Loss: 0.0058, Validation Loss: 0.0001, Validation accuracy: 0.5020\n",
            "Epoch 8/10\n",
            "Loss: 0.00585569953545928, Accuracy 0.375\n",
            "Loss: 0.00574510358273983, Accuracy 0.4765625\n",
            "Epoch: 8, Training Loss: 0.0058, Validation Loss: 0.0001, Validation accuracy: 0.5020\n",
            "Epoch 9/10\n",
            "Loss: 0.005769249517470598, Accuracy 0.46875\n",
            "Loss: 0.005809423513710499, Accuracy 0.4609375\n",
            "Epoch: 9, Training Loss: 0.0058, Validation Loss: 0.0001, Validation accuracy: 0.5020\n",
            "Epoch 10/10\n",
            "Loss: 0.005583089776337147, Accuracy 0.53125\n",
            "Loss: 0.005544555373489857, Accuracy 0.5625\n",
            "Epoch: 10, Training Loss: 0.0058, Validation Loss: 0.0001, Validation accuracy: 0.5020\n",
            "Test Loss: 0.0000, Test accuracy: 0.4949\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwjD1PctLiJu"
      },
      "source": [
        "## Training a linear classifier with a pretrained embedding\n",
        "\n",
        "Load a GloVe pretrained embedding instead"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "And_mbLGLiJw"
      },
      "source": [
        "TEXT.build_vocab(\n",
        "    train_data, max_size=vocab_size, vectors=\"glove.6B.100d\", vectors_cache=torch_cache\n",
        ")\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "RiiFdgJ4LiJw"
      },
      "source": [
        "class GloVeEmbeddingNet(nn.Module):\n",
        "    def __init__(self, seq_length, freeze=True):\n",
        "        super().__init__()\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "        # Define `embedding_dim` from vocabulary and the pretrained `embedding`.\n",
        "        self.embedding_dim = TEXT.vocab.vectors.shape[1]\n",
        "        self.embedding = nn.Embedding.from_pretrained(\n",
        "            TEXT.vocab.vectors,\n",
        "            freeze = freeze\n",
        "        )\n",
        "\n",
        "\n",
        "        self.l1 = nn.Linear(self.seq_length * self.embedding_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # `x` is of size batch_size * seq_length\n",
        "\n",
        "        # `embedded` is of size batch_size * seq_length * embedding_dim\n",
        "        embedded = self.embedding(x)\n",
        "\n",
        "        # `flatten` is of size batch_size * (seq_length * embedding_dim)\n",
        "        flatten = embedded.view(-1, self.seq_length * self.embedding_dim)\n",
        "\n",
        "        # L1 is of size batch_size\n",
        "        return self.l1(flatten).squeeze()"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pf3xYavLiJw"
      },
      "source": [
        "## Use pretrained embedding without fine-tuning\n",
        "\n",
        "Define model and freeze the embedding\n",
        "glove_embedding_net1 = ...\n",
        "\n",
        "\n",
        "tr.model = glove_embedding_net1\n",
        "tr.train()\n",
        "tr.test()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HB734LJcLiJw"
      },
      "source": [
        "## Fine-tuning the pretrained embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 1,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7JZjpvqLiJw",
        "outputId": "7a4d97a0-3b33-4686-d7c4-737ae461cc1f"
      },
      "source": [
        "# Define model and don't freeze embedding weights\n",
        "glove_embedding_net2 = GloVeEmbeddingNet(seq_length, freeze=True)\n",
        "\n",
        "\n",
        "tr.model = glove_embedding_net2\n",
        "tr.train()\n",
        "tr.test()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "Loss: 0.0057112774811685085, Accuracy 0.4609375\n",
            "Loss: 0.005576574709266424, Accuracy 0.515625\n",
            "Epoch: 1, Training Loss: 0.0056, Validation Loss: 0.0001, Validation accuracy: 0.4886\n",
            "Epoch 2/10\n",
            "Loss: 0.005742017179727554, Accuracy 0.4765625\n",
            "Loss: 0.005288746207952499, Accuracy 0.5390625\n",
            "Epoch: 2, Training Loss: 0.0056, Validation Loss: 0.0001, Validation accuracy: 0.4886\n",
            "Epoch 3/10\n",
            "Loss: 0.00556968804448843, Accuracy 0.5\n",
            "Loss: 0.005532684735953808, Accuracy 0.484375\n",
            "Epoch: 3, Training Loss: 0.0056, Validation Loss: 0.0001, Validation accuracy: 0.4886\n",
            "Epoch 4/10\n",
            "Loss: 0.005418528337031603, Accuracy 0.4921875\n",
            "Loss: 0.005492957308888435, Accuracy 0.5234375\n",
            "Epoch: 4, Training Loss: 0.0056, Validation Loss: 0.0001, Validation accuracy: 0.4886\n",
            "Epoch 5/10\n",
            "Loss: 0.005505882669240236, Accuracy 0.5078125\n",
            "Loss: 0.005705522373318672, Accuracy 0.4609375\n",
            "Epoch: 5, Training Loss: 0.0056, Validation Loss: 0.0001, Validation accuracy: 0.4886\n",
            "Epoch 6/10\n",
            "Loss: 0.005556231364607811, Accuracy 0.484375\n",
            "Loss: 0.005567025393247604, Accuracy 0.484375\n",
            "Epoch: 6, Training Loss: 0.0056, Validation Loss: 0.0001, Validation accuracy: 0.4886\n",
            "Epoch 7/10\n",
            "Loss: 0.005481456406414509, Accuracy 0.5234375\n",
            "Loss: 0.005541983991861343, Accuracy 0.53125\n",
            "Epoch: 7, Training Loss: 0.0056, Validation Loss: 0.0001, Validation accuracy: 0.4886\n",
            "Epoch 8/10\n",
            "Loss: 0.005472722928971052, Accuracy 0.4765625\n",
            "Loss: 0.005578741431236267, Accuracy 0.5078125\n",
            "Epoch: 8, Training Loss: 0.0056, Validation Loss: 0.0001, Validation accuracy: 0.4886\n",
            "Epoch 9/10\n",
            "Loss: 0.005477690603584051, Accuracy 0.484375\n",
            "Loss: 0.005564838647842407, Accuracy 0.5546875\n",
            "Epoch: 9, Training Loss: 0.0056, Validation Loss: 0.0001, Validation accuracy: 0.4886\n",
            "Epoch 10/10\n",
            "Loss: 0.005547245964407921, Accuracy 0.515625\n",
            "Loss: 0.005641627125442028, Accuracy 0.453125\n",
            "Epoch: 10, Training Loss: 0.0056, Validation Loss: 0.0001, Validation accuracy: 0.4886\n",
            "Test Loss: 0.0000, Test accuracy: 0.4958\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z16yzTrJLiJw"
      },
      "source": [
        "## Recurrent neural network with frozen pretrained embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tA2kz5s9LiJw",
        "outputId": "888dbb9b-d439-457a-be1c-46fca681c551"
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        # Define frozen pretrained embedding\n",
        "        self.embedding = nn.Embedding.from_pretrained(TEXT.vocab.vectors, freeze=True)\n",
        "\n",
        "        # Get size of input `x_t` from `embedding`\n",
        "        self.embedding_size = self.embedding.embedding_dim\n",
        "        self.input_size = self.embedding_size\n",
        "\n",
        "        # Size of hidden state `h_t`\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Define a GRU, don't forget to set `batch_first` to True\n",
        "        self.gru = nn.GRU(\n",
        "            input_size = self.input_size,\n",
        "            hidden_size = self.hidden_size,\n",
        "            batch_first = True\n",
        "        )\n",
        "        self.linear = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x, h0=None):\n",
        "        # `x` is of size batch_size * seq_length and `h0` is of size 1\n",
        "        # * `batch_size` * `hidden_size`\n",
        "\n",
        "        # Define first hidden state in not provided\n",
        "        if h0 is None:\n",
        "            # Get batch and define `h0` which is of size 1 *\n",
        "            # `batch_size` * `hidden_size`\n",
        "            batch_size = x.size(0)\n",
        "            h0 = torch.zeros(1, batch_size, self.hidden_size)\n",
        "\n",
        "\n",
        "        # `embedded` is of size `batch_size` * `seq_length` *\n",
        "        # `embedding_dim`\n",
        "        embedded = self.embedding(x)\n",
        "\n",
        "        # Define `output` and `hidden`\n",
        "        _, hn = self.gru(embedded, h0)\n",
        "\n",
        "        # `output` is of size `batch_size` * `seq_length` * `hidden_size`\n",
        "        return self.linear(hn.squeeze()).squeeze()\n",
        "\n",
        "\n",
        "\n",
        "rnn = RNN(hidden_size=100)\n",
        "\n",
        "tr.model = rnn\n",
        "tr.train()\n",
        "tr.test()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "Loss: 0.005376743618398905, Accuracy 0.5625\n",
            "Loss: 0.005456102080643177, Accuracy 0.5078125\n",
            "Epoch: 1, Training Loss: 0.0055, Validation Loss: 0.0001, Validation accuracy: 0.5062\n",
            "Epoch 2/10\n",
            "Loss: 0.005370131228119135, Accuracy 0.578125\n",
            "Loss: 0.005671412218362093, Accuracy 0.4140625\n",
            "Epoch: 2, Training Loss: 0.0055, Validation Loss: 0.0001, Validation accuracy: 0.5062\n",
            "Epoch 3/10\n",
            "Loss: 0.005534551106393337, Accuracy 0.4765625\n",
            "Loss: 0.005465097259730101, Accuracy 0.53125\n",
            "Epoch: 3, Training Loss: 0.0055, Validation Loss: 0.0001, Validation accuracy: 0.5062\n",
            "Epoch 4/10\n",
            "Loss: 0.00545708741992712, Accuracy 0.515625\n",
            "Loss: 0.005335585214197636, Accuracy 0.5546875\n",
            "Epoch: 4, Training Loss: 0.0055, Validation Loss: 0.0001, Validation accuracy: 0.5062\n",
            "Epoch 5/10\n",
            "Loss: 0.005467582959681749, Accuracy 0.53125\n",
            "Loss: 0.005444640293717384, Accuracy 0.5078125\n",
            "Epoch: 5, Training Loss: 0.0055, Validation Loss: 0.0001, Validation accuracy: 0.5062\n",
            "Epoch 6/10\n",
            "Loss: 0.005460340064018965, Accuracy 0.515625\n",
            "Loss: 0.005580349359661341, Accuracy 0.4453125\n",
            "Epoch: 6, Training Loss: 0.0055, Validation Loss: 0.0001, Validation accuracy: 0.5062\n",
            "Epoch 7/10\n",
            "Loss: 0.005481676664203405, Accuracy 0.4921875\n",
            "Loss: 0.005555158015340567, Accuracy 0.484375\n",
            "Epoch: 7, Training Loss: 0.0055, Validation Loss: 0.0001, Validation accuracy: 0.5062\n",
            "Epoch 8/10\n",
            "Loss: 0.005446816328912973, Accuracy 0.5234375\n",
            "Loss: 0.00545900221914053, Accuracy 0.53125\n",
            "Epoch: 8, Training Loss: 0.0055, Validation Loss: 0.0001, Validation accuracy: 0.5062\n",
            "Epoch 9/10\n",
            "Loss: 0.00533866323530674, Accuracy 0.53125\n",
            "Loss: 0.005414869170635939, Accuracy 0.5234375\n",
            "Epoch: 9, Training Loss: 0.0055, Validation Loss: 0.0001, Validation accuracy: 0.5062\n",
            "Epoch 10/10\n",
            "Loss: 0.005451200529932976, Accuracy 0.5\n",
            "Loss: 0.005450757686048746, Accuracy 0.4921875\n",
            "Epoch: 10, Training Loss: 0.0055, Validation Loss: 0.0001, Validation accuracy: 0.5062\n",
            "Test Loss: 0.0000, Test accuracy: 0.4994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sql1irb3LiJx"
      },
      "source": [
        "## CNN based text classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCcOwNWpLiJy"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, freeze=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding_dim = TEXT.vocab.vectors.shape[1]\n",
        "        self.embedding = nn.Embedding.from_pretrained(TEXT.vocab.vectors, freeze=freeze)\n",
        "        self.conv_0 = nn.Conv2d(\n",
        "            in_channels=1, out_channels=100, kernel_size=(3, self.embedding_dim)\n",
        "        )\n",
        "        self.conv_1 = nn.Conv2d(\n",
        "            in_channels=1, out_channels=100, kernel_size=(4, self.embedding_dim)\n",
        "        )\n",
        "        self.conv_2 = nn.Conv2d(\n",
        "            in_channels=1, out_channels=100, kernel_size=(5, self.embedding_dim)\n",
        "        )\n",
        "        self.linear = nn.Linear(3 * 100, 1)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # `x` is of size batch_size * seq_length\n",
        "        embedded = self.embedding(x)\n",
        "\n",
        "        # `embedded` is of size batch_size * seq_length * embedding_dim\n",
        "        # and should be of size batch_size * (n_channels=1) *\n",
        "        # seq_length * embedding_dim\n",
        "        # Unsqueeze `embedded`\n",
        "\n",
        "        # `embedded` is now of size batch_size * 1 * seq_length *\n",
        "        # embedding_dim  before convolution and should be of size\n",
        "        # batch_size * (out_channels = 100) * (seq_length - kernel_size[0]) * 1\n",
        "        # after convolution.\n",
        "        # Implement the convolution layer\n",
        "\n",
        "        # Non-linearity step, we use Relu activation\n",
        "        # Implement the relu non-linearity\n",
        "\n",
        "        # Max-pooling layer: pooling along whole sequence\n",
        "        # Implement max pooling\n",
        "\n",
        "        # Dropout on concatenated pooled features\n",
        "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))\n",
        "\n",
        "        # Linear layer\n",
        "        return self.linear(cat).squeeze()"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "iPSoNDaRLiJy",
        "outputId": "b31d089c-13b2-4e98-a70e-783c394baea0"
      },
      "source": [
        "cnn = CNN(vocab_size_)\n",
        "tr.model = cnn\n",
        "tr.train()\n",
        "tr.test()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-96f66b7be690>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-57352fd5bd71>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-54-611807092c92>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# Dropout on concatenated pooled features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# Linear layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pooled_0' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOI4KGf3LiJy"
      },
      "source": [
        "## Test function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-35TupeLiJy"
      },
      "source": [
        "def predict_sentiment(model, sentence):\n",
        "    \"Predict sentiment of given sentence according to model\"\n",
        "\n",
        "    tokens = TEXT.preprocess(sentence)\n",
        "    padded = TEXT.pad([tokens])\n",
        "    tensor = TEXT.numericalize(padded)\n",
        "    prediction = torch.sigmoid(model(tensor))\n",
        "    return prediction.item()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}