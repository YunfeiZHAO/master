{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding and RNN for sentiment analysis\n",
    "\n",
    "The goal of the following notebook is to predict whether a written\n",
    "critic about a movie is positive or negative. For that we will try\n",
    "three models. A simple linear model on the word embeddings, a\n",
    "recurrent neural network and a CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchtext import data, datasets, vocab\n",
    "\n",
    "# Used to cache pretrained embeddings\n",
    "import appdirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data\n",
    "\n",
    "First run the following block that will install spacy for tokenizing\n",
    "and download an IMDB dataset and a GloVe embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user spacy\n",
    "!spacy download en --user\n",
    "\n",
    "# Download IMDB data\n",
    "torch_cache = appdirs.user_cache_dir(\"pytorch\")\n",
    "datasets.IMDB.download(torch_cache)\n",
    "\n",
    "# Download GloVe word embedding\n",
    "vocab.GloVe(name=\"6B\", dim=\"100\", cache=torch_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables\n",
    "\n",
    "First let's define a few variables. `vocab_size` is the size of the\n",
    "vocabulary (ie number of known words) we will use. `embedding_dim` is\n",
    "the dimension of the vector space used to embed all the words of the\n",
    "vocabulary. `seq_length` is the maximum length of a sequence,\n",
    "`batch_size` is the size of the batches used in stochastic\n",
    "optimization algorithms and `n_epochs` the number of times we are\n",
    "going thought the entire training set during the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Define a few variables\n",
    "vocab_size = ...\n",
    "embedding_dim = ...\n",
    "seq_length = ...\n",
    "batch_size = ...\n",
    "n_epochs = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The IMDB dataset\n",
    "\n",
    "We use SpaCy and torchtext to create training, validation and testing\n",
    "datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "spacy_en = spacy.load(\"en\")\n",
    "tokenize = \"spacy\"\n",
    "\n",
    "# Declare the fields\n",
    "TEXT = data.Field(\n",
    "    tokenize=tokenize, fix_length=seq_length, lower=True, batch_first=True\n",
    ")\n",
    "LABEL = data.LabelField(sequential=False, dtype=torch.float)\n",
    "\n",
    "# IMDB dataset is already divided into train and test\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL, root=torch_cache)\n",
    "\n",
    "# Creating a validation dataset from the training one\n",
    "train_data, valid_data = train_data.split(split_ratio=0.8)\n",
    "\n",
    "print(f\"Number of training examples: {len(train_data)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data)}\")\n",
    "print(f\"Number of testing examples: {len(test_data)}\")\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size=vocab_size)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define true vocabulary size because there are two more tokens\n",
    "vocab_size_ = len(TEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(len(TEXT.vocab))\n",
    "print(TEXT.vocab.itos[:10])\n",
    "print(train_data.examples[0].text[:seq_length])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Training a linear classifier with an embedding\n",
    "\n",
    "We first test a simple linear classifier on the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, seq_length):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # Define an embedding of `vocab_size` words into a vector space\n",
    "        # of dimension `embedding_dim`.\n",
    "\n",
    "        self.embedding = ...\n",
    "\n",
    "\n",
    "        # Define a linear layer from dimension `seq_length` *\n",
    "        # `embedding_dim` to 1.\n",
    "\n",
    "        self.l1 = ...\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # `x` is of size `batch_size` * `seq_length`\n",
    "\n",
    "        # Compute the embedding `embedded` of the batch `x`. `embedded` is\n",
    "        # of size `batch_size` * `seq_length` * `embedding_dim`\n",
    "\n",
    "        embedded = ...\n",
    "\n",
    "\n",
    "        # Flatten the embedded words and feed it to the linear layer.\n",
    "        # `flatten` is of size `batch_size` * (`seq_length` * `embedding_dim`)\n",
    "\n",
    "        flatten = ...\n",
    "\n",
    "\n",
    "        # Apply the linear layer and return a squeezed version\n",
    "        # `l1` is of size `batch_size`\n",
    "\n",
    "        return ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "We need to implement an accuracy function to be used in the `Trainer`\n",
    "class (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    # `predictions` and `labels` are both tensors of same length\n",
    "\n",
    "    # Implement accuracy\n",
    "    return ...\n",
    "\n",
    "\n",
    "\n",
    "assert accuracy(torch.Tensor([1, -2, 3]), torch.Tensor([1, 0, 1])) == 1\n",
    "assert accuracy(torch.Tensor([1, -2, -3]), torch.Tensor([1, 0, 1])) == 2 / 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement now a `Trainer` class that takes care of the learning\n",
    "process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_epochs=10,\n",
    "        model=None,\n",
    "        optimizer=None,\n",
    "        criterion=None,\n",
    "        train_data=None,\n",
    "        valid_data=None,\n",
    "        test_data=None,\n",
    "        batch_size=None,\n",
    "    ):\n",
    "        self.n_epochs = n_epochs\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.train_iterator = data.BucketIterator(train_data, batch_size=batch_size)\n",
    "        self.test_data = test_data\n",
    "        self.valid_data = valid_data\n",
    "\n",
    "    def train(self):\n",
    "        # model might have changed so redefine `optimizer`\n",
    "        optimizer = self.optimizer(self.model.parameters())\n",
    "\n",
    "        for epoch in range(1, self.n_epochs + 1):\n",
    "            print(f\"Epoch {epoch}/{self.n_epochs}\")\n",
    "\n",
    "            running_loss = 0.0\n",
    "            self.model.train()  # turn on training mode\n",
    "            for step, batch in enumerate(self.train_iterator):\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                if step % 150 == 0:\n",
    "                    acc = accuracy(predictions, batch.label)\n",
    "                    print(f\"Loss: {loss.item()/batch_size}, Accuracy {acc}\")\n",
    "\n",
    "            epoch_loss = running_loss / len(train_data)\n",
    "\n",
    "            # Calculate the validation loss for this epoch\n",
    "            self.model.eval()  # turn on evaluation mode\n",
    "            full_batch = data.Batch(self.valid_data, self.valid_data)\n",
    "            # Define the accuracy `valid_acc` and the loss `valid_loss` on `full_batch`\n",
    "            predictions = ...\n",
    "            valid_loss = ...\n",
    "            valid_acc = ...\n",
    "\n",
    "\n",
    "            print(\n",
    "                f\"Epoch: {epoch}, Training Loss: {epoch_loss:.4f}, Validation Loss: {valid_loss:.4f}, Validation accuracy: {valid_acc:.4f}\"\n",
    "            )\n",
    "\n",
    "    def test(self):\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()  # turn on evaluation mode\n",
    "            full_batch = data.Batch(self.test_data, self.test_data)\n",
    "            # Define the accuracy `test_acc` and the loss `test_loss` on `full_batch`\n",
    "            predictions = ...\n",
    "            test_loss = ...\n",
    "            test_acc = ...\n",
    "\n",
    "\n",
    "            print(f\"Test Loss: {test_loss:.4f}, Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "\n",
    "embedding_net = EmbeddingNet(vocab_size_, embedding_dim, seq_length)\n",
    "optimizer = optim.Adam\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "tr = Trainer(\n",
    "    n_epochs=n_epochs,\n",
    "    model=embedding_net,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    train_data=train_data,\n",
    "    test_data=test_data,\n",
    "    valid_data=valid_data,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "tr.train()\n",
    "tr.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a linear classifier with a pretrained embedding\n",
    "\n",
    "Load a GloVe pretrained embedding instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(\n",
    "    train_data, max_size=vocab_size, vectors=\"glove.6B.100d\", vectors_cache=torch_cache\n",
    ")\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class GloVeEmbeddingNet(nn.Module):\n",
    "    def __init__(self, seq_length, freeze=True):\n",
    "        super().__init__()\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        # Define `embedding_dim` from vocabulary and the pretrained `embedding`.\n",
    "        self.embedding_dim = ...\n",
    "        self.embedding = nn.Embedding.from_pretrained(...)\n",
    "\n",
    "\n",
    "        self.l1 = nn.Linear(self.seq_length * self.embedding_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # `x` is of size batch_size * seq_length\n",
    "\n",
    "        # `embedded` is of size batch_size * seq_length * embedding_dim\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # `flatten` is of size batch_size * (seq_length * embedding_dim)\n",
    "        flatten = embedded.view(-1, self.seq_length * self.embedding_dim)\n",
    "\n",
    "        # L1 is of size batch_size\n",
    "        return self.l1(flatten).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use pretrained embedding without fine-tuning\n",
    "\n",
    "Define model and freeze the embedding\n",
    "glove_embedding_net1 = ...\n",
    "\n",
    "\n",
    "tr.model = glove_embedding_net1\n",
    "tr.train()\n",
    "tr.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the pretrained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define model and don't freeze embedding weights\n",
    "glove_embedding_net2 = ...\n",
    "\n",
    "\n",
    "tr.model = glove_embedding_net2\n",
    "tr.train()\n",
    "tr.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural network with frozen pretrained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        # Define frozen pretrained embedding\n",
    "        self.embedding = nn.Embedding.from_pretrained(TEXT.vocab.vectors, freeze=True)\n",
    "\n",
    "        # Get size of input `x_t` from `embedding`\n",
    "        self.embedding_size = self.embedding.embedding_dim\n",
    "        self.input_size = self.embedding_size\n",
    "\n",
    "        # Size of hidden state `h_t`\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Define a GRU, don't forget to set `batch_first` to True\n",
    "        self.gru = nn.GRU(...)\n",
    "\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x, h0=None):\n",
    "        # `x` is of size batch_size * seq_length and `h0` is of size 1\n",
    "        # * `batch_size` * `hidden_size`\n",
    "\n",
    "        # Define first hidden state in not provided\n",
    "        if h0 is None:\n",
    "            # Get batch and define `h0` which is of size 1 *\n",
    "            # `batch_size` * `hidden_size`\n",
    "            batch_size = ...\n",
    "            h0 = torch.zeros(...)\n",
    "\n",
    "\n",
    "        # `embedded` is of size `batch_size` * `seq_length` *\n",
    "        # `embedding_dim`\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # Define `output` and `hidden`\n",
    "\n",
    "        # `output` is of size `batch_size` * `seq_length` * `hidden_size`\n",
    "        return ...\n",
    "\n",
    "\n",
    "\n",
    "rnn = RNN(hidden_size=100)\n",
    "\n",
    "tr.model = rnn\n",
    "tr.train()\n",
    "tr.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN based text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, freeze=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_dim = TEXT.vocab.vectors.shape[1]\n",
    "        self.embedding = nn.Embedding.from_pretrained(TEXT.vocab.vectors, freeze=freeze)\n",
    "        self.conv_0 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=100, kernel_size=(3, self.embedding_dim)\n",
    "        )\n",
    "        self.conv_1 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=100, kernel_size=(4, self.embedding_dim)\n",
    "        )\n",
    "        self.conv_2 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=100, kernel_size=(5, self.embedding_dim)\n",
    "        )\n",
    "        self.linear = nn.Linear(3 * 100, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # `x` is of size batch_size * seq_length\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # `embedded` is of size batch_size * seq_length * embedding_dim\n",
    "        # and should be of size batch_size * (n_channels=1) *\n",
    "        # seq_length * embedding_dim\n",
    "        # Unsqueeze `embedded`\n",
    "\n",
    "        # `embedded` is now of size batch_size * 1 * seq_length *\n",
    "        # embedding_dim  before convolution and should be of size\n",
    "        # batch_size * (out_channels = 100) * (seq_length - kernel_size[0]) * 1\n",
    "        # after convolution.\n",
    "        # Implement the convolution layer\n",
    "\n",
    "        # Non-linearity step, we use Relu activation\n",
    "        # Implement the relu non-linearity\n",
    "\n",
    "        # Max-pooling layer: pooling along whole sequence\n",
    "        # Implement max pooling\n",
    "\n",
    "        # Dropout on concatenated pooled features\n",
    "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))\n",
    "\n",
    "        # Linear layer\n",
    "        return self.linear(cat).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN(vocab_size_)\n",
    "tr.model = cnn\n",
    "tr.train()\n",
    "tr.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, sentence):\n",
    "    \"Predict sentiment of given sentence according to model\"\n",
    "\n",
    "    tokens = TEXT.preprocess(sentence)\n",
    "    padded = TEXT.pad([tokens])\n",
    "    tensor = TEXT.numericalize(padded)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    return prediction.item()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
