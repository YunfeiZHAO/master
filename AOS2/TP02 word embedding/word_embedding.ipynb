{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW model trained on \"20000 lieues sous les mers\"\n",
    "## Needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.fr import French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#python -m spacy download fr_core_news_sm\n",
    "spacy_fr = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tokenizer for the french language\n",
    "tokenizer = French().Defaults.create_tokenizer()\n",
    "\n",
    "with open(\"data/20_000_lieues_sous_les_mers.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    document = tokenizer(f.read())\n",
    "\n",
    "# Define a filtered set of tokens by iterating on `document`\n",
    "# filter on blank, comma etc\n",
    "tokens = [tok.text for tok in document if tok.is_alpha]\n",
    "\n",
    "\n",
    "# Make a list of unique tokens and dictionary that maps tokens to\n",
    "# their index in that list.\n",
    "idx2tok = []\n",
    "tok2idx = {}\n",
    "i =  0\n",
    "for tok in tokens:\n",
    "    if tok not in idx2tok:\n",
    "        idx2tok.append(tok)\n",
    "        tok2idx[tok] = i\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The continuous bag of words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        # Define an Embedding module (`nn.Embedding`) and a linear\n",
    "        # transform (`nn.Linear`) without bias.\n",
    "        self.embeddings = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        self.U_transpose = nn.Linear(self.embedding_size, self.vocab_size, bias=False) #from model we dont want biais\n",
    "\n",
    "\n",
    "    def forward(self, context):\n",
    "        # Implements the forward pass\n",
    "        # `context` is of size `batch_size` * NGRAMS\n",
    "\n",
    "        # `e_i` is of size `batch_size` * NGRAMS * `embedding_size`\n",
    "        e_i = self.embeddings(context)\n",
    "\n",
    "\n",
    "        # `e_bar` is of size `batch_size` * `embedding_size`\n",
    "        e_bar = torch.mean(e_i, 1) # average all the words belonging to the context\n",
    "\n",
    "\n",
    "        # `UT_e_bar` is of size `batch_size` * `vocab_size`\n",
    "        UT_e_bar = self.U_transpose(e_bar)\n",
    "\n",
    "\n",
    "        # Use `F.log_softmax` function\n",
    "        return F.log_softmax(UT_e_bar, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "# Set the size of vocabulary and size of embedding\n",
    "VOCAB_SIZE = len(idx2tok)\n",
    "EMBEDDING_SIZE = 64\n",
    "\n",
    "# Create a Continuous bag of words model\n",
    "cbow = CBOW(VOCAB_SIZE, EMBEDDING_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_iterator(token_list, ngrams):\n",
    "    \"\"\"Generates sucessive N-grams from a list of tokens.\"\"\"\n",
    "\n",
    "    # Creates `ngrams` lists shifted to the left\n",
    "    token_list_shifts = [token_list[i:] for i in range(ngrams)]\n",
    "    for ngram in zip(*token_list_shifts):\n",
    "        # Get indexes of tokens\n",
    "        idxs = [tok2idx[tok] for tok in ngram]\n",
    "\n",
    "        # Get center element in `idxs`\n",
    "        center = idxs.pop(ngrams // 2)\n",
    "\n",
    "        # Yield the index of center word and indexes of context words\n",
    "        # as a Numpy array (for Pytorch to automatically convert it to\n",
    "        # a Tensor).\n",
    "        yield center, np.array(idxs)\n",
    "\n",
    "\n",
    "# Create center, context data\n",
    "NGRAMS = 5\n",
    "ngrams = list(ngrams_iterator(tokens, NGRAMS))\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "data = torch.utils.data.DataLoader(ngrams, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn CBOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Use the Adam algorithm on the parameters of `cbow` with a learning\n",
    "# rate of 0.01\n",
    "optimizer = optim.Adam(cbow.parameters(), lr=0.01)\n",
    "\n",
    "# Use a negative log-likelyhood loss from the `nn` submodule\n",
    "nll_loss = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (0/10), batch: (0/541), loss: 9.669973373413086\n",
      "Epoch (0/10), batch: (20/541), loss: 9.35292911529541\n",
      "Epoch (0/10), batch: (40/541), loss: 8.901827812194824\n",
      "Epoch (0/10), batch: (60/541), loss: 8.625412940979004\n",
      "Epoch (0/10), batch: (80/541), loss: 8.406564712524414\n",
      "Epoch (0/10), batch: (100/541), loss: 8.243134498596191\n",
      "Epoch (0/10), batch: (120/541), loss: 8.11301040649414\n",
      "Epoch (0/10), batch: (140/541), loss: 7.999739646911621\n",
      "Epoch (0/10), batch: (160/541), loss: 7.9211344718933105\n",
      "Epoch (0/10), batch: (180/541), loss: 7.847839832305908\n",
      "Epoch (0/10), batch: (200/541), loss: 7.782588481903076\n",
      "Epoch (0/10), batch: (220/541), loss: 7.7256011962890625\n",
      "Epoch (0/10), batch: (240/541), loss: 7.674098968505859\n",
      "Epoch (0/10), batch: (260/541), loss: 7.628327369689941\n",
      "Epoch (0/10), batch: (280/541), loss: 7.588433742523193\n",
      "Epoch (0/10), batch: (300/541), loss: 7.546175003051758\n",
      "Epoch (0/10), batch: (320/541), loss: 7.510591506958008\n",
      "Epoch (0/10), batch: (340/541), loss: 7.471757888793945\n",
      "Epoch (0/10), batch: (360/541), loss: 7.438479423522949\n",
      "Epoch (0/10), batch: (380/541), loss: 7.4073591232299805\n",
      "Epoch (0/10), batch: (400/541), loss: 7.376068115234375\n",
      "Epoch (0/10), batch: (420/541), loss: 7.34536075592041\n",
      "Epoch (0/10), batch: (440/541), loss: 7.317350387573242\n",
      "Epoch (0/10), batch: (460/541), loss: 7.2909064292907715\n",
      "Epoch (0/10), batch: (480/541), loss: 7.272579193115234\n",
      "Epoch (0/10), batch: (500/541), loss: 7.2493181228637695\n",
      "Epoch (0/10), batch: (520/541), loss: 7.229650974273682\n",
      "Epoch (0/10), batch: (540/541), loss: 7.211051940917969\n",
      "0/10 loss 7.21\n",
      "Epoch (1/10), batch: (0/541), loss: 5.594717979431152\n",
      "Epoch (1/10), batch: (20/541), loss: 5.427988052368164\n",
      "Epoch (1/10), batch: (40/541), loss: 5.409219741821289\n",
      "Epoch (1/10), batch: (60/541), loss: 5.412914752960205\n",
      "Epoch (1/10), batch: (80/541), loss: 5.425615310668945\n",
      "Epoch (1/10), batch: (100/541), loss: 5.44812536239624\n",
      "Epoch (1/10), batch: (120/541), loss: 5.455225467681885\n",
      "Epoch (1/10), batch: (140/541), loss: 5.4590277671813965\n",
      "Epoch (1/10), batch: (160/541), loss: 5.473263740539551\n",
      "Epoch (1/10), batch: (180/541), loss: 5.480339050292969\n",
      "Epoch (1/10), batch: (200/541), loss: 5.490598678588867\n",
      "Epoch (1/10), batch: (220/541), loss: 5.493226051330566\n",
      "Epoch (1/10), batch: (240/541), loss: 5.490021705627441\n",
      "Epoch (1/10), batch: (260/541), loss: 5.498436450958252\n",
      "Epoch (1/10), batch: (280/541), loss: 5.505510330200195\n",
      "Epoch (1/10), batch: (300/541), loss: 5.511463165283203\n",
      "Epoch (1/10), batch: (320/541), loss: 5.515937328338623\n",
      "Epoch (1/10), batch: (340/541), loss: 5.518491744995117\n",
      "Epoch (1/10), batch: (360/541), loss: 5.524948596954346\n",
      "Epoch (1/10), batch: (380/541), loss: 5.529881477355957\n",
      "Epoch (1/10), batch: (400/541), loss: 5.531018257141113\n",
      "Epoch (1/10), batch: (420/541), loss: 5.529502868652344\n",
      "Epoch (1/10), batch: (440/541), loss: 5.533807754516602\n",
      "Epoch (1/10), batch: (460/541), loss: 5.535632133483887\n",
      "Epoch (1/10), batch: (480/541), loss: 5.5383219718933105\n",
      "Epoch (1/10), batch: (500/541), loss: 5.539975166320801\n",
      "Epoch (1/10), batch: (520/541), loss: 5.5419158935546875\n",
      "Epoch (1/10), batch: (540/541), loss: 5.54594612121582\n",
      "1/10 loss 5.55\n",
      "Epoch (2/10), batch: (0/541), loss: 4.342548370361328\n",
      "Epoch (2/10), batch: (20/541), loss: 4.326253414154053\n",
      "Epoch (2/10), batch: (40/541), loss: 4.305792331695557\n",
      "Epoch (2/10), batch: (60/541), loss: 4.326208114624023\n",
      "Epoch (2/10), batch: (80/541), loss: 4.3498215675354\n",
      "Epoch (2/10), batch: (100/541), loss: 4.378652095794678\n",
      "Epoch (2/10), batch: (120/541), loss: 4.395199298858643\n",
      "Epoch (2/10), batch: (140/541), loss: 4.403338432312012\n",
      "Epoch (2/10), batch: (160/541), loss: 4.417744159698486\n",
      "Epoch (2/10), batch: (180/541), loss: 4.430290222167969\n",
      "Epoch (2/10), batch: (200/541), loss: 4.444521903991699\n",
      "Epoch (2/10), batch: (220/541), loss: 4.455803394317627\n",
      "Epoch (2/10), batch: (240/541), loss: 4.468325614929199\n",
      "Epoch (2/10), batch: (260/541), loss: 4.480001926422119\n",
      "Epoch (2/10), batch: (280/541), loss: 4.4950737953186035\n",
      "Epoch (2/10), batch: (300/541), loss: 4.504487991333008\n",
      "Epoch (2/10), batch: (320/541), loss: 4.514339447021484\n",
      "Epoch (2/10), batch: (340/541), loss: 4.523618698120117\n",
      "Epoch (2/10), batch: (360/541), loss: 4.534316062927246\n",
      "Epoch (2/10), batch: (380/541), loss: 4.541505336761475\n",
      "Epoch (2/10), batch: (400/541), loss: 4.549391269683838\n",
      "Epoch (2/10), batch: (420/541), loss: 4.557177543640137\n",
      "Epoch (2/10), batch: (440/541), loss: 4.566908836364746\n",
      "Epoch (2/10), batch: (460/541), loss: 4.573532581329346\n",
      "Epoch (2/10), batch: (480/541), loss: 4.584254741668701\n",
      "Epoch (2/10), batch: (500/541), loss: 4.592764854431152\n",
      "Epoch (2/10), batch: (520/541), loss: 4.6016974449157715\n",
      "Epoch (2/10), batch: (540/541), loss: 4.609205722808838\n",
      "2/10 loss 4.61\n",
      "Epoch (3/10), batch: (0/541), loss: 3.4320318698883057\n",
      "Epoch (3/10), batch: (20/541), loss: 3.485746383666992\n",
      "Epoch (3/10), batch: (40/541), loss: 3.5140228271484375\n",
      "Epoch (3/10), batch: (60/541), loss: 3.5323846340179443\n",
      "Epoch (3/10), batch: (80/541), loss: 3.5292022228240967\n",
      "Epoch (3/10), batch: (100/541), loss: 3.5434200763702393\n",
      "Epoch (3/10), batch: (120/541), loss: 3.559821128845215\n",
      "Epoch (3/10), batch: (140/541), loss: 3.585393190383911\n",
      "Epoch (3/10), batch: (160/541), loss: 3.606234073638916\n",
      "Epoch (3/10), batch: (180/541), loss: 3.6267683506011963\n",
      "Epoch (3/10), batch: (200/541), loss: 3.636430263519287\n",
      "Epoch (3/10), batch: (220/541), loss: 3.653825521469116\n",
      "Epoch (3/10), batch: (240/541), loss: 3.668403148651123\n",
      "Epoch (3/10), batch: (260/541), loss: 3.679485321044922\n",
      "Epoch (3/10), batch: (280/541), loss: 3.6908090114593506\n",
      "Epoch (3/10), batch: (300/541), loss: 3.7071173191070557\n",
      "Epoch (3/10), batch: (320/541), loss: 3.720747947692871\n",
      "Epoch (3/10), batch: (340/541), loss: 3.7370176315307617\n",
      "Epoch (3/10), batch: (360/541), loss: 3.7484569549560547\n",
      "Epoch (3/10), batch: (380/541), loss: 3.760641574859619\n",
      "Epoch (3/10), batch: (400/541), loss: 3.7716121673583984\n",
      "Epoch (3/10), batch: (420/541), loss: 3.7832725048065186\n",
      "Epoch (3/10), batch: (440/541), loss: 3.795257091522217\n",
      "Epoch (3/10), batch: (460/541), loss: 3.806863307952881\n",
      "Epoch (3/10), batch: (480/541), loss: 3.8180248737335205\n",
      "Epoch (3/10), batch: (500/541), loss: 3.8296918869018555\n",
      "Epoch (3/10), batch: (520/541), loss: 3.8390746116638184\n",
      "Epoch (3/10), batch: (540/541), loss: 3.848388195037842\n",
      "3/10 loss 3.85\n",
      "Epoch (4/10), batch: (0/541), loss: 2.879134178161621\n",
      "Epoch (4/10), batch: (20/541), loss: 2.8495700359344482\n",
      "Epoch (4/10), batch: (40/541), loss: 2.8743138313293457\n",
      "Epoch (4/10), batch: (60/541), loss: 2.896699905395508\n",
      "Epoch (4/10), batch: (80/541), loss: 2.9140803813934326\n",
      "Epoch (4/10), batch: (100/541), loss: 2.947894811630249\n",
      "Epoch (4/10), batch: (120/541), loss: 2.9699413776397705\n",
      "Epoch (4/10), batch: (140/541), loss: 2.985994338989258\n",
      "Epoch (4/10), batch: (160/541), loss: 2.9982333183288574\n",
      "Epoch (4/10), batch: (180/541), loss: 3.016770124435425\n",
      "Epoch (4/10), batch: (200/541), loss: 3.0302460193634033\n",
      "Epoch (4/10), batch: (220/541), loss: 3.0431454181671143\n",
      "Epoch (4/10), batch: (240/541), loss: 3.0589962005615234\n",
      "Epoch (4/10), batch: (260/541), loss: 3.072678327560425\n",
      "Epoch (4/10), batch: (280/541), loss: 3.085437536239624\n",
      "Epoch (4/10), batch: (300/541), loss: 3.0980350971221924\n",
      "Epoch (4/10), batch: (320/541), loss: 3.1144344806671143\n",
      "Epoch (4/10), batch: (340/541), loss: 3.1291253566741943\n",
      "Epoch (4/10), batch: (360/541), loss: 3.1410274505615234\n",
      "Epoch (4/10), batch: (380/541), loss: 3.1525540351867676\n",
      "Epoch (4/10), batch: (400/541), loss: 3.1670639514923096\n",
      "Epoch (4/10), batch: (420/541), loss: 3.1801583766937256\n",
      "Epoch (4/10), batch: (440/541), loss: 3.190424919128418\n",
      "Epoch (4/10), batch: (460/541), loss: 3.201889991760254\n",
      "Epoch (4/10), batch: (480/541), loss: 3.2129600048065186\n",
      "Epoch (4/10), batch: (500/541), loss: 3.2232742309570312\n",
      "Epoch (4/10), batch: (520/541), loss: 3.23272705078125\n",
      "Epoch (4/10), batch: (540/541), loss: 3.2443227767944336\n",
      "4/10 loss 3.24\n",
      "Epoch (5/10), batch: (0/541), loss: 2.3925821781158447\n",
      "Epoch (5/10), batch: (20/541), loss: 2.425642490386963\n",
      "Epoch (5/10), batch: (40/541), loss: 2.4599218368530273\n",
      "Epoch (5/10), batch: (60/541), loss: 2.4876484870910645\n",
      "Epoch (5/10), batch: (80/541), loss: 2.512683153152466\n",
      "Epoch (5/10), batch: (100/541), loss: 2.5268077850341797\n",
      "Epoch (5/10), batch: (120/541), loss: 2.537332057952881\n",
      "Epoch (5/10), batch: (140/541), loss: 2.5549659729003906\n",
      "Epoch (5/10), batch: (160/541), loss: 2.578979969024658\n",
      "Epoch (5/10), batch: (180/541), loss: 2.5955865383148193\n",
      "Epoch (5/10), batch: (200/541), loss: 2.608564853668213\n",
      "Epoch (5/10), batch: (220/541), loss: 2.622856378555298\n",
      "Epoch (5/10), batch: (240/541), loss: 2.6323866844177246\n",
      "Epoch (5/10), batch: (260/541), loss: 2.6426117420196533\n",
      "Epoch (5/10), batch: (280/541), loss: 2.653616428375244\n",
      "Epoch (5/10), batch: (300/541), loss: 2.6628143787384033\n",
      "Epoch (5/10), batch: (320/541), loss: 2.672358274459839\n",
      "Epoch (5/10), batch: (340/541), loss: 2.6851253509521484\n",
      "Epoch (5/10), batch: (360/541), loss: 2.6951277256011963\n",
      "Epoch (5/10), batch: (380/541), loss: 2.7045631408691406\n",
      "Epoch (5/10), batch: (400/541), loss: 2.7151782512664795\n",
      "Epoch (5/10), batch: (420/541), loss: 2.7260420322418213\n",
      "Epoch (5/10), batch: (440/541), loss: 2.7371327877044678\n",
      "Epoch (5/10), batch: (460/541), loss: 2.74946928024292\n",
      "Epoch (5/10), batch: (480/541), loss: 2.7606041431427\n",
      "Epoch (5/10), batch: (500/541), loss: 2.7675623893737793\n",
      "Epoch (5/10), batch: (520/541), loss: 2.777143716812134\n",
      "Epoch (5/10), batch: (540/541), loss: 2.7850422859191895\n",
      "5/10 loss 2.79\n",
      "Epoch (6/10), batch: (0/541), loss: 2.1973724365234375\n",
      "Epoch (6/10), batch: (20/541), loss: 2.1140315532684326\n",
      "Epoch (6/10), batch: (40/541), loss: 2.116891860961914\n",
      "Epoch (6/10), batch: (60/541), loss: 2.1273059844970703\n",
      "Epoch (6/10), batch: (80/541), loss: 2.1411681175231934\n",
      "Epoch (6/10), batch: (100/541), loss: 2.1592612266540527\n",
      "Epoch (6/10), batch: (120/541), loss: 2.165987014770508\n",
      "Epoch (6/10), batch: (140/541), loss: 2.182634115219116\n",
      "Epoch (6/10), batch: (160/541), loss: 2.195784568786621\n",
      "Epoch (6/10), batch: (180/541), loss: 2.2130095958709717\n",
      "Epoch (6/10), batch: (200/541), loss: 2.22709321975708\n",
      "Epoch (6/10), batch: (220/541), loss: 2.239777088165283\n",
      "Epoch (6/10), batch: (240/541), loss: 2.255575180053711\n",
      "Epoch (6/10), batch: (260/541), loss: 2.2680532932281494\n",
      "Epoch (6/10), batch: (280/541), loss: 2.2829480171203613\n",
      "Epoch (6/10), batch: (300/541), loss: 2.2908456325531006\n",
      "Epoch (6/10), batch: (320/541), loss: 2.3032643795013428\n",
      "Epoch (6/10), batch: (340/541), loss: 2.3169310092926025\n",
      "Epoch (6/10), batch: (360/541), loss: 2.32698130607605\n",
      "Epoch (6/10), batch: (380/541), loss: 2.337395429611206\n",
      "Epoch (6/10), batch: (400/541), loss: 2.3496785163879395\n",
      "Epoch (6/10), batch: (420/541), loss: 2.359015703201294\n",
      "Epoch (6/10), batch: (440/541), loss: 2.370471954345703\n",
      "Epoch (6/10), batch: (460/541), loss: 2.3801302909851074\n",
      "Epoch (6/10), batch: (480/541), loss: 2.393440008163452\n",
      "Epoch (6/10), batch: (500/541), loss: 2.4064433574676514\n",
      "Epoch (6/10), batch: (520/541), loss: 2.419222831726074\n",
      "Epoch (6/10), batch: (540/541), loss: 2.4302120208740234\n",
      "6/10 loss 2.43\n",
      "Epoch (7/10), batch: (0/541), loss: 1.9062656164169312\n",
      "Epoch (7/10), batch: (20/541), loss: 1.8568313121795654\n",
      "Epoch (7/10), batch: (40/541), loss: 1.8514350652694702\n",
      "Epoch (7/10), batch: (60/541), loss: 1.8701671361923218\n",
      "Epoch (7/10), batch: (80/541), loss: 1.8861409425735474\n",
      "Epoch (7/10), batch: (100/541), loss: 1.9066426753997803\n",
      "Epoch (7/10), batch: (120/541), loss: 1.9238101243972778\n",
      "Epoch (7/10), batch: (140/541), loss: 1.9283643960952759\n",
      "Epoch (7/10), batch: (160/541), loss: 1.9456775188446045\n",
      "Epoch (7/10), batch: (180/541), loss: 1.9634605646133423\n",
      "Epoch (7/10), batch: (200/541), loss: 1.974127173423767\n",
      "Epoch (7/10), batch: (220/541), loss: 1.9846184253692627\n",
      "Epoch (7/10), batch: (240/541), loss: 1.9912959337234497\n",
      "Epoch (7/10), batch: (260/541), loss: 2.0025484561920166\n",
      "Epoch (7/10), batch: (280/541), loss: 2.0157787799835205\n",
      "Epoch (7/10), batch: (300/541), loss: 2.029033899307251\n",
      "Epoch (7/10), batch: (320/541), loss: 2.040980100631714\n",
      "Epoch (7/10), batch: (340/541), loss: 2.0500473976135254\n",
      "Epoch (7/10), batch: (360/541), loss: 2.062530994415283\n",
      "Epoch (7/10), batch: (380/541), loss: 2.073901653289795\n",
      "Epoch (7/10), batch: (400/541), loss: 2.087026596069336\n",
      "Epoch (7/10), batch: (420/541), loss: 2.0981762409210205\n",
      "Epoch (7/10), batch: (440/541), loss: 2.1080050468444824\n",
      "Epoch (7/10), batch: (460/541), loss: 2.119443655014038\n",
      "Epoch (7/10), batch: (480/541), loss: 2.130434036254883\n",
      "Epoch (7/10), batch: (500/541), loss: 2.1396493911743164\n",
      "Epoch (7/10), batch: (520/541), loss: 2.1484756469726562\n",
      "Epoch (7/10), batch: (540/541), loss: 2.1578402519226074\n",
      "7/10 loss 2.16\n",
      "Epoch (8/10), batch: (0/541), loss: 1.5300874710083008\n",
      "Epoch (8/10), batch: (20/541), loss: 1.6466245651245117\n",
      "Epoch (8/10), batch: (40/541), loss: 1.655789852142334\n",
      "Epoch (8/10), batch: (60/541), loss: 1.6472861766815186\n",
      "Epoch (8/10), batch: (80/541), loss: 1.6864266395568848\n",
      "Epoch (8/10), batch: (100/541), loss: 1.6929116249084473\n",
      "Epoch (8/10), batch: (120/541), loss: 1.7100781202316284\n",
      "Epoch (8/10), batch: (140/541), loss: 1.7211503982543945\n",
      "Epoch (8/10), batch: (160/541), loss: 1.7383028268814087\n",
      "Epoch (8/10), batch: (180/541), loss: 1.7500602006912231\n",
      "Epoch (8/10), batch: (200/541), loss: 1.7639999389648438\n",
      "Epoch (8/10), batch: (220/541), loss: 1.7793337106704712\n",
      "Epoch (8/10), batch: (240/541), loss: 1.794287919998169\n",
      "Epoch (8/10), batch: (260/541), loss: 1.8038928508758545\n",
      "Epoch (8/10), batch: (280/541), loss: 1.8164247274398804\n",
      "Epoch (8/10), batch: (300/541), loss: 1.8256442546844482\n",
      "Epoch (8/10), batch: (320/541), loss: 1.8353744745254517\n",
      "Epoch (8/10), batch: (340/541), loss: 1.844089388847351\n",
      "Epoch (8/10), batch: (360/541), loss: 1.8529627323150635\n",
      "Epoch (8/10), batch: (380/541), loss: 1.862351655960083\n",
      "Epoch (8/10), batch: (400/541), loss: 1.8731328248977661\n",
      "Epoch (8/10), batch: (420/541), loss: 1.8854681253433228\n",
      "Epoch (8/10), batch: (440/541), loss: 1.8966741561889648\n",
      "Epoch (8/10), batch: (460/541), loss: 1.9036983251571655\n",
      "Epoch (8/10), batch: (480/541), loss: 1.9139461517333984\n",
      "Epoch (8/10), batch: (500/541), loss: 1.925015926361084\n",
      "Epoch (8/10), batch: (520/541), loss: 1.9361733198165894\n",
      "Epoch (8/10), batch: (540/541), loss: 1.945759892463684\n",
      "8/10 loss 1.95\n",
      "Epoch (9/10), batch: (0/541), loss: 1.7281156778335571\n",
      "Epoch (9/10), batch: (20/541), loss: 1.5482145547866821\n",
      "Epoch (9/10), batch: (40/541), loss: 1.5423212051391602\n",
      "Epoch (9/10), batch: (60/541), loss: 1.547925591468811\n",
      "Epoch (9/10), batch: (80/541), loss: 1.5517171621322632\n",
      "Epoch (9/10), batch: (100/541), loss: 1.5528967380523682\n",
      "Epoch (9/10), batch: (120/541), loss: 1.5668600797653198\n",
      "Epoch (9/10), batch: (140/541), loss: 1.5765093564987183\n",
      "Epoch (9/10), batch: (160/541), loss: 1.5879451036453247\n",
      "Epoch (9/10), batch: (180/541), loss: 1.6050012111663818\n",
      "Epoch (9/10), batch: (200/541), loss: 1.6109763383865356\n",
      "Epoch (9/10), batch: (220/541), loss: 1.6210956573486328\n",
      "Epoch (9/10), batch: (240/541), loss: 1.629071831703186\n",
      "Epoch (9/10), batch: (260/541), loss: 1.6390084028244019\n",
      "Epoch (9/10), batch: (280/541), loss: 1.6518186330795288\n",
      "Epoch (9/10), batch: (300/541), loss: 1.6615350246429443\n",
      "Epoch (9/10), batch: (320/541), loss: 1.6729589700698853\n",
      "Epoch (9/10), batch: (340/541), loss: 1.6824487447738647\n",
      "Epoch (9/10), batch: (360/541), loss: 1.6938356161117554\n",
      "Epoch (9/10), batch: (380/541), loss: 1.7056424617767334\n",
      "Epoch (9/10), batch: (400/541), loss: 1.7155102491378784\n",
      "Epoch (9/10), batch: (420/541), loss: 1.7234669923782349\n",
      "Epoch (9/10), batch: (440/541), loss: 1.7344123125076294\n",
      "Epoch (9/10), batch: (460/541), loss: 1.7441799640655518\n",
      "Epoch (9/10), batch: (480/541), loss: 1.7523657083511353\n",
      "Epoch (9/10), batch: (500/541), loss: 1.760625958442688\n",
      "Epoch (9/10), batch: (520/541), loss: 1.7694147825241089\n",
      "Epoch (9/10), batch: (540/541), loss: 1.7782872915267944\n",
      "9/10 loss 1.78\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "try:\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0\n",
    "        for i, (center, context) in enumerate(data):\n",
    "            # Reset the gradients of the computational graph\n",
    "            cbow.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            # nlll_w_hat is of size ``batch_size*vocab_size``\n",
    "            nll_w_hat = cbow.forward(context)\n",
    "\n",
    "\n",
    "            # Compute negative log-likelyhood loss averaged over the\n",
    "            # mini-batch\n",
    "            loss = nll_loss(nll_w_hat, center)\n",
    "\n",
    "\n",
    "            # Backward pass to compute gradients of each parameter\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient descent step according to the chosen optimizer\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.data\n",
    "\n",
    "            if i % 20 == 0:\n",
    "                loss_avg = float(total_loss / (i + 1))\n",
    "                print(\n",
    "                    f\"Epoch ({epoch}/{EPOCHS}), batch: ({i}/{len(data)}), loss: {loss_avg}\"\n",
    "                )\n",
    "\n",
    "        # Print average loss after each epoch\n",
    "        loss_avg = float(total_loss / len(data))\n",
    "        print(\"{}/{} loss {:.2f}\".format(epoch, EPOCHS, loss_avg))\n",
    "\n",
    "        # Predict if `predict_center_word` is implemented\n",
    "        try:\n",
    "            left_words = [\"le\", \"capitaine\"]\n",
    "            right_words = [\"me\", \"dit\"]\n",
    "            word = predict_center_word(cbow, *left_words, *right_words)[0]\n",
    "            print(\" \".join(left_words + [word] + right_words))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopped!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.629248055780836"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.log(1/len(idx2tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_center_word_idx(cbow, *context_words_idx, k=10):\n",
    "    \"\"\"Return k-best center words given indexes of context words.\"\"\"\n",
    "\n",
    "    # Create a fake minibatch containing just one example\n",
    "    fake_minibatch = torch.LongTensor(context_words_idx).unsqueeze(0)\n",
    "    \n",
    "    # forward propagate thought the CBOW model\n",
    "    dist_center = cbow(fake_minibatch).squeeze(0)\n",
    "    \n",
    "    # Retrieve top k-best indexes using 'torch.topk'\n",
    "    _, best_idxs = torch.topk(dist_center, k=k)\n",
    "    \n",
    "    # Return actual tokens using `idx2tok`\n",
    "    return [idx2tok[idx] for idx in best_idxs]\n",
    "\n",
    "\n",
    "def predict_center_word(cbow, *context_words, k=10):\n",
    "    \"\"\"Return k-best center words given context words.\"\"\"\n",
    "\n",
    "    idxs = [tok2idx[tok] for tok in context_words]\n",
    "    return predict_center_word_idx(cbow, *idxs, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pieds', 'milles', 'mètres', 'trous', 'six', 'roches', 'lieues', 'livres', 'Pendant', 'de']\n",
      "['par', 'du', 'de', 'deux', 'et', 'Dans', 'sept', 'dans', 'soit', 'commun']\n",
      "['Farragut', 'Nautilus', 'tuerait', 'cria', 'ci', 'sur', 'frictionnions', 'habituel', 'second', 'attendre']\n"
     ]
    }
   ],
   "source": [
    "print(predict_center_word(cbow, \"vingt\", \"mille\", \"sous\", \"les\"))\n",
    "print(predict_center_word(cbow, \"mille\", \"lieues\", \"les\", \"mers\"))\n",
    "print(predict_center_word(cbow, \"le\", \"commandant\", \"fut\", \"le\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens by decreasing frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = np.zeros((len(idx2tok),), int)\n",
    "for tok in tokens:\n",
    "    freq[tok2idx[tok]] += 1\n",
    "idxs = freq.argsort()[::-1]\n",
    "words_decreasing_freq = list(zip(np.array(idx2tok)[idxs], freq[idxs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import WordEmbeddingsKeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = WordEmbeddingsKeyedVectors(vector_size=EMBEDDING_SIZE)\n",
    "m.add(idx2tok, cbow.embeddings.weight.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('comatules', 0.5313119292259216),\n",
       " ('embranchements', 0.529474139213562),\n",
       " ('Porcelaines', 0.5240614414215088),\n",
       " ('tatous', 0.5176289081573486),\n",
       " ('corsetés', 0.5093542337417603),\n",
       " ('côtes', 0.5086833238601685),\n",
       " ('entrailles', 0.4992835521697998),\n",
       " ('immodérément', 0.497053325176239),\n",
       " ('perspective', 0.4962666928768158),\n",
       " ('regardant', 0.492682546377182)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.most_similar(\"bassins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
