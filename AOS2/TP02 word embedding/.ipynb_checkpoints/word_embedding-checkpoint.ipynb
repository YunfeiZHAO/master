{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW model trained on \"20000 lieues sous les mers\"\n",
    "## Needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.fr import French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# python -m spacy download fr_core_news_sm\n",
    "spacy_fr = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tokenizer for the french language\n",
    "tokenizer = French().Defaults.create_tokenizer()\n",
    "\n",
    "with open(\"data/20_000_lieues_sous_les_mers.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    document = tokenizer(f.read())\n",
    "\n",
    "# Define a filtered set of tokens by iterating on `document`\n",
    "tokens = ...\n",
    "\n",
    "\n",
    "# Make a list of unique tokens and dictionary that maps tokens to\n",
    "# their index in that list.\n",
    "idx2tok = []\n",
    "tok2idx = {}\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The continuous bag of words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        # Define an Embedding module (`nn.Embedding`) and a linear\n",
    "        # transform (`nn.Linear`) without bias.\n",
    "        self.embeddings = ...\n",
    "        self.U_transpose = ...\n",
    "\n",
    "\n",
    "    def forward(self, context):\n",
    "        # Implements the forward pass\n",
    "        # `context` is of size `batch_size` * NGRAMS\n",
    "\n",
    "        # `e_i` is of size `batch_size` * NGRAMS * `embedding_size`\n",
    "        e_i = ...\n",
    "\n",
    "\n",
    "        # `e_bar` is of size `batch_size` * `embedding_size`\n",
    "        e_bar = ...\n",
    "\n",
    "\n",
    "        # `UT_e_bar` is of size `embedding_size` * `vocab_size`\n",
    "        UT_e_bar = ...\n",
    "\n",
    "\n",
    "        # Use `F.log_softmax` function\n",
    "        return ...\n",
    "\n",
    "\n",
    "\n",
    "# Set the size of vocabulary and size of embedding\n",
    "VOCAB_SIZE = len(idx2tok)\n",
    "EMBEDDING_SIZE = 64\n",
    "\n",
    "# Create a Continuous bag of words model\n",
    "cbow = CBOW(VOCAB_SIZE, EMBEDDING_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_iterator(token_list, ngrams):\n",
    "    \"\"\"Generates sucessive N-grams from a list of tokens.\"\"\"\n",
    "\n",
    "    # Creates `ngrams` lists shifted to the left\n",
    "    token_list_shifts = [token_list[i:] for i in range(ngrams)]\n",
    "    for ngram in zip(*token_list_shifts):\n",
    "        # Get indexes of tokens\n",
    "        idxs = [tok2idx[tok] for tok in ngram]\n",
    "\n",
    "        # Get center element in `idxs`\n",
    "        center = idxs.pop(ngrams // 2)\n",
    "\n",
    "        # Yield the index of center word and indexes of context words\n",
    "        # as a Numpy array (for Pytorch to automatically convert it to\n",
    "        # a Tensor).\n",
    "        yield center, np.array(idxs)\n",
    "\n",
    "\n",
    "# Create center, context data\n",
    "NGRAMS = 5\n",
    "ngrams = list(ngrams_iterator(tokens, NGRAMS))\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "data = torch.utils.data.DataLoader(ngrams, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn CBOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Use the Adam algorithm on the parameters of `cbow` with a learning\n",
    "# rate of 0.01\n",
    "optimizer = ...\n",
    "\n",
    "\n",
    "# Use a negative log-likelyhood loss from the `nn` submodule\n",
    "nll_loss = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "try:\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0\n",
    "        for i, (center, context) in enumerate(data):\n",
    "            # Reset the gradients of the computational graph\n",
    "            ...\n",
    "\n",
    "            # Forward pass\n",
    "            nll_w_hat = ...\n",
    "\n",
    "\n",
    "            # Compute negative log-likelyhood loss averaged over the\n",
    "            # mini-batch\n",
    "            loss = ...\n",
    "\n",
    "\n",
    "            # Backward pass to compute gradients of each parameter\n",
    "            ...\n",
    "\n",
    "            # Gradient descent step according to the chosen optimizer\n",
    "            ...\n",
    "\n",
    "            total_loss += loss.data\n",
    "\n",
    "            if i % 20 == 0:\n",
    "                loss_avg = float(total_loss / (i + 1))\n",
    "                print(\n",
    "                    f\"Epoch ({epoch}/{EPOCHS}), batch: ({i}/{len(data)}), loss: {loss_avg}\"\n",
    "                )\n",
    "\n",
    "        # Print average loss after each epoch\n",
    "        loss_avg = float(total_loss / len(data))\n",
    "        print(\"{}/{} loss {:.2f}\".format(epoch, EPOCHS, loss_avg))\n",
    "\n",
    "        # Predict if `predict_center_word` is implemented\n",
    "        try:\n",
    "            left_words = [\"le\", \"capitaine\"]\n",
    "            right_words = [\"me\", \"dit\"]\n",
    "            word = predict_center_word(cbow, *left_words, *right_words)[0]\n",
    "            print(\" \".join(left_words + [word] + right_words))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopped!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_center_word_idx(cbow, *context_words_idx, k=10):\n",
    "    \"\"\"Return k-best center words given indexes of context words.\"\"\"\n",
    "\n",
    "    fake_minibatch = torch.LongTensor(context_words_idx).unsqueeze(0)\n",
    "    dist_center = cbow(fake_minibatch).squeeze()\n",
    "    _, best_idxs = torch.topk(dist_center, k=k)\n",
    "    return [idx2tok[idx] for idx in best_idxs]\n",
    "\n",
    "\n",
    "def predict_center_word(cbow, *context_words, k=10):\n",
    "    \"\"\"Return k-best center words given context words.\"\"\"\n",
    "\n",
    "    idxs = [tok2idx[tok] for tok in context_words]\n",
    "    return cbow.predict_center_word_idx(*idxs, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow.predict_center_word(\"vingt\", \"mille\", \"sous\", \"les\")\n",
    "cbow.predict_center_word(\"mille\", \"lieues\", \"les\", \"mers\")\n",
    "cbow.predict_center_word(\"le\", \"commandant\", \"fut\", \"le\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
