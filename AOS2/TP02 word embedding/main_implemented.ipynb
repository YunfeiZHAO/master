{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW model trained on \"20000 lieues sous les mers\"\n",
    "## Needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.fr import French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# python -m spacy download fr_core_news_sm\n",
    "spacy_fr = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tokenizer for the french language\n",
    "tokenizer = French().Defaults.create_tokenizer()\n",
    "\n",
    "with open(\"data/20_000_lieues_sous_les_mers.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    document = tokenizer(f.read())\n",
    "\n",
    "# Define a filtered set of tokens by iterating on `document`\n",
    "tokens = [tok.text.lower() for tok in document if tok.is_alpha]\n",
    "\n",
    "# Make a list of unique tokens and dictionary that maps tokens to\n",
    "# their index in that list.\n",
    "tok2idx = {}\n",
    "idx2tok = []\n",
    "i = 0\n",
    "for tok in tokens:\n",
    "    if tok not in tok2idx:\n",
    "        tok2idx[tok] = i\n",
    "        idx2tok.append(tok)\n",
    "        i += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The continuous bag of words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        # Define an Embedding module (`nn.Embedding`) and a linear\n",
    "        # transform (`nn.Linear`) without bias.\n",
    "        self.embeddings = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        self.U_transpose = nn.Linear(self.embedding_size, self.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, context):\n",
    "        # Implements the forward pass\n",
    "        # `context` is of size `batch_size` * NGRAMS\n",
    "\n",
    "        # `e_i` is of size `batch_size` * NGRAMS * `embedding_size`\n",
    "        e_i = self.embeddings(context)\n",
    "\n",
    "        # `e_bar` is of size `batch_size` * `embedding_size`\n",
    "        e_bar = torch.mean(e_i, 1)\n",
    "\n",
    "        # `UT_e_bar` is of size `embedding_size` * `vocab_size`\n",
    "        UT_e_bar = self.U_transpose(e_bar)\n",
    "\n",
    "        # Use `F.log_softmax` function\n",
    "        return F.log_softmax(UT_e_bar, dim=1)\n",
    "\n",
    "\n",
    "# Set the size of vocabulary and size of embedding\n",
    "VOCAB_SIZE = len(idx2tok)\n",
    "EMBEDDING_SIZE = 64\n",
    "\n",
    "# Create a Continuous bag of words model\n",
    "cbow = CBOW(VOCAB_SIZE, EMBEDDING_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_iterator(token_list, ngrams):\n",
    "    \"\"\"Generates sucessive N-grams from a list of tokens.\"\"\"\n",
    "\n",
    "    # Creates `ngrams` lists shifted to the left\n",
    "    token_list_shifts = [token_list[i:] for i in range(ngrams)]\n",
    "    for ngram in zip(*token_list_shifts):\n",
    "        # Get indexes of tokens\n",
    "        idxs = [tok2idx[tok] for tok in ngram]\n",
    "\n",
    "        # Get center element in `idxs`\n",
    "        center = idxs.pop(ngrams // 2)\n",
    "\n",
    "        # Yield the index of center word and indexes of context words\n",
    "        # as a Numpy array (for Pytorch to automatically convert it to\n",
    "        # a Tensor).\n",
    "        yield center, np.array(idxs)\n",
    "\n",
    "\n",
    "# Create center, context data\n",
    "NGRAMS = 5\n",
    "ngrams = list(ngrams_iterator(tokens, NGRAMS))\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "data = torch.utils.data.DataLoader(ngrams, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn CBOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Adam algorithm on the parameters of `cbow` with a learning\n",
    "# rate of 0.01\n",
    "optimizer = optim.Adam(cbow.parameters(), lr=0.01)\n",
    "\n",
    "# Use a negative log-likelyhood loss from the `nn` submodule\n",
    "nll_loss = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "try:\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0\n",
    "        for i, (center, context) in enumerate(data):\n",
    "            # Reset the gradients of the computational graph\n",
    "            cbow.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            nll_w_hat = cbow(context)\n",
    "\n",
    "            # Compute negative log-likelyhood loss averaged over the\n",
    "            # mini-batch\n",
    "            loss = nll_loss(nll_w_hat, center)\n",
    "\n",
    "            # Backward pass to compute gradients of each parameter\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient descent step according to the chosen optimizer\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.data\n",
    "\n",
    "            if i % 20 == 0:\n",
    "                loss_avg = float(total_loss / (i + 1))\n",
    "                print(\n",
    "                    f\"Epoch ({epoch}/{EPOCHS}), batch: ({i}/{len(data)}), loss: {loss_avg}\"\n",
    "                )\n",
    "\n",
    "        # Print average loss after each epoch\n",
    "        loss_avg = float(total_loss / len(data))\n",
    "        print(\"{}/{} loss {:.2f}\".format(epoch, EPOCHS, loss_avg))\n",
    "\n",
    "        # Predict if `predict_center_word` is implemented\n",
    "        try:\n",
    "            left_words = [\"le\", \"capitaine\"]\n",
    "            right_words = [\"me\", \"dit\"]\n",
    "            word = predict_center_word(cbow, *left_words, *right_words)[0]\n",
    "            print(\" \".join(left_words + [word] + right_words))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopped!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction functions\n",
    "\n",
    "Now that the model is learned we can give it a context it has never\n",
    "seen and see what center word it predicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_center_word_idx(cbow, *context_words_idx, k=10):\n",
    "    \"\"\"Return k-best center words given indexes of context words.\"\"\"\n",
    "\n",
    "    # Create a fake minibatch containing just one example\n",
    "    fake_minibatch = torch.LongTensor(context_words_idx).unsqueeze(0)\n",
    "\n",
    "    # Forward propagate throught the CBOW model\n",
    "    dist_center = cbow(fake_minibatch).squeeze()\n",
    "\n",
    "    # Retrieve top k-best indexes using `torch.topk`\n",
    "    _, best_idxs = torch.topk(dist_center, k=k)\n",
    "\n",
    "    # Return actual tokens using `idx2tok`\n",
    "    return [idx2tok[idx] for idx in best_idxs]\n",
    "\n",
    "\n",
    "def predict_center_word(cbow, *context_words, k=10):\n",
    "    \"\"\"Return k-best center words given context words.\"\"\"\n",
    "\n",
    "    idxs = [tok2idx[tok] for tok in context_words]\n",
    "    return predict_center_word_idx(cbow, *idxs, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_center_word(cbow, \"vingt\", \"mille\", \"sous\", \"les\")\n",
    "predict_center_word(cbow, \"mille\", \"lieues\", \"les\", \"mers\")\n",
    "predict_center_word(cbow, \"le\", \"commandant\", \"fut\", \"le\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the embedding\n",
    "\n",
    "Tokens by decreasing frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "freq = np.zeros((len(idx2tok),), int)\n",
    "for tok in tokens:\n",
    "    freq[tok2idx[tok]] += 1\n",
    "\n",
    "idxs = freq.argsort()[::-1]\n",
    "words_decreasing_freq = list(zip(np.array(idx2tok)[idxs], freq[idxs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "We use the library `gensim` to easily compute most similar words for\n",
    "the embedding we just learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import WordEmbeddingsKeyedVectors\n",
    "\n",
    "m = WordEmbeddingsKeyedVectors(vector_size=EMBEDDING_SIZE)\n",
    "m.add(idx2tok, cbow.embeddings.weight.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "You can now test most similar words for, for example \"lieues\",\n",
    "\"mers\", \"professeur\"... You can look at `words_decreasing_freq` to\n",
    "test most frequent tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.most_similar(\"lieues\")\n",
    "m.most_similar(\"professeur\")\n",
    "m.most_similar(\"mers\")\n",
    "m.most_similar(\"a\")\n",
    "m.most_similar(\"été\")\n",
    "m.most_similar(\"ma\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
